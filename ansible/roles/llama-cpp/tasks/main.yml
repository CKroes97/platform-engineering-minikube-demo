- name: Set up envirionment for llama-cpp
  ansible.builtin.include_tasks: setup_environment.yml

- name: Install llama-cpp
  ansible.builtin.include_tasks: llama_cpp.yml
  vars:
    repo_url: "https://github.com/ggml-org/llama.cpp.git"
    repo_dest: "/opt/llama.cpp"
    build_dir: "{{ repo_dest }}/build"
    cmake_generator: "Ninja"          # or "Unix Makefiles"
    cmake_build_type: "Release"
    enable_cuda: "ON"
    cuda_architectures: "89"

- name: Run llama-server
  ansible.builtin.include_tasks: llama-server.yml
  vars:
    repo_dir: "/opt/llama.cpp"
    model_dir: "/opt/llama_models"
    model_spec: "ggml-org/gpt-oss-20b-GGUF"
    service_user: "llama"
    service_name: "llama-server"
    service_port: 39443
    threads: 8
    gpu_layers: 20
    ctx_size: 4096
