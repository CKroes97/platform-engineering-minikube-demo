---
- name: Create directories for LocalAI data and models
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: '0755'
  loop:
    - "{{ localai_models_dir }}"
    - "{{ localai_data_dir }}"

- name: Check if LocalAI binary exists
  ansible.builtin.stat:
    path: "{{ localai_install_dir }}/local-ai"
  register: localai_bin

- name: Download LocalAI binary
  ansible.builtin.get_url:
    url: "https://github.com/mudler/LocalAI/releases/download/v{{ localai_version }}/local-ai-v3.7.0-linux-amd64"
    dest: "{{ localai_install_dir }}/local-ai"
    mode: "0755"
  when: not localai_bin.stat.exists

- name: Install llama.cpp CUDA backend via LocalAI CLI
  ansible.builtin.command: local-ai backend install llama-cpp-cuda
  register: install_backend
  changed_when: "'Backend installed successfully' in install_backend.stdout"
  failed_when: "'Error' in install_backend.stderr"

- name: Ensure LocalAI systemd service file exists
  ansible.builtin.copy:
    dest: "/etc/systemd/system/{{ localai_service_name }}.service"
    content: |
      [Unit]
      Description=LocalAI self-hosted API (Hugging Face Model)
      After=network.target docker.service

      [Service]
      ExecStart={{ localai_install_dir }}/local-ai run \
        --models huggingface://{{ huggingface_model_id }} \
        --address 0.0.0.0:{{ localai_api_port }}
      WorkingDirectory={{ localai_data_dir }}
      Environment="LOCALAI_FORCE_META_BACKEND_CAPABILITY=gpu"
      Restart=always
      User=root

      [Install]
      WantedBy=multi-user.target
    mode: '0644'

- name: Reload systemd daemon
  ansible.builtin.systemd:
    daemon_reload: true

- name: Ensure LocalAI service is enabled and running
  ansible.builtin.service:
    name: "{{ localai_service_name }}"
    state: started
    enabled: true
