---
- name: Clone llama.cpp repository
  ansible.builtin.git:
    repo: "{{ repo_url }}"
    dest: "{{ repo_dest }}"
    version: "master"
    force: true

- name: Create build directory
  ansible.builtin.file:
    path: "{{ build_dir }}"
    state: directory
    mode: "0755"

- name: Run CMake to configure build (with CUDA)
  ansible.builtin.command:
    cmd: >
      cmake
      -S "{{ repo_dest }}"
      -B "{{ build_dir }}"
      -G "{{ cmake_generator }}"
      -DCMAKE_BUILD_TYPE={{ cmake_build_type }}
      -DGGML_CUDA={{ enable_cuda }}
      -DCMAKE_CUDA_ARCHITECTURES="{{ cuda_architectures }}"
      -DCMAKE_C_COMPILER=/usr/bin/gcc-13
      -DCMAKE_CXX_COMPILER=/usr/bin/g++-13
      -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc
  args:
    creates: "{{ build_dir }}/bin/llama-server"
    chdir: "{{ repo_dest }}"

- name: Build llama.cpp binaries
  ansible.builtin.command:
    cmd: >
      cmake --build "{{ build_dir }}" --config {{ cmake_build_type }}
  args:
    creates: "{{ build_dir }}/bin/llama-server"
    chdir: "{{ build_dir }}"
