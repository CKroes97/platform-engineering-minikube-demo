---
- name: Set up requirements
  ansible.builtin.include_tasks: reqs.yml
  vars:
    venv_dir: "/opt/localai/.venv"

- name: Set up cuda
  ansible.builtin.include_tasks: cuda.yml

- name: Build llama.cpp with CUDA support
  ansible.builtin.include_tasks: llama-cpp.yml
  vars:
    repo_url: "https://github.com/ggml-org/llama.cpp.git"
    repo_dest: "/opt/llama.cpp"
    build_dir: "/var/lib/localai/backends"
    cmake_generator: "Ninja"          # or "Unix Makefiles"
    cmake_build_type: "Release"
    enable_cuda: "ON"
    cuda_architectures: "89"

- name: Run localai service
  ansible.builtin.include_tasks: localai.yml
  vars:
    python_version: "3.11"
    venv_dir: "/opt/localai/.venv"
    localai_version: "3.7.0"
    localai_install_dir: "/usr/local/bin"
    localai_service_name: "localai"
    localai_api_port: 39443
    localai_data_dir: "/var/lib/localai"
    localai_models_dir: "/var/lib/localai/models"
    localai_backends_dir: "/var/lib/localai/backends"
    huggingface_model_id: "ggml-org/gpt-oss-20b-GGUF/gpt-oss-20b-mxfp4.gguf"
    backend: "localai@cuda12-llama-cpp"
