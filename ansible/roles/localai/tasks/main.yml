- name: Set up requirements
  ansible.builtin.include_tasks: reqs.yml

- name: Set up cuda
  ansible.builtin.include_tasks: cuda.yml

- name: Run llama-server
  ansible.builtin.include_tasks: llama-server.yml
  vars:
    repo_dir: "/opt/llama.cpp"
    model_dir: "/opt/llama_models"
    model_spec: "ggml-org/gpt-oss-20b-GGUF"
    service_user: "llama"
    service_name: "llama-server"
    service_port: 39443
    threads: 8
    gpu_layers: 20
    ctx_size: 4096
